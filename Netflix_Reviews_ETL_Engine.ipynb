{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5cd01c-614e-4112-829b-44aa4d8405b3",
   "metadata": {},
   "source": [
    "## Netflix Reviews ETL and Sentiment Engine\n",
    "## Author: Oyefejo Olaseni Ifeoluwa\n",
    "## Objective: Automate the sync, cleaning and categorization of Netflix App reviews.\n",
    "## Stack: Python (Pandas), SQL (SQLAlchemy), NLP (BERTopic), LLM (Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7854f35c-70f9-42e1-9cda-552433c4bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary librabries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from pathlib import Path\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "import joblib  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7914e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION and LOGGING\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "DATASET = \"ashishkumarak/netflix-reviews-playstore-daily-updated\"\n",
    "LOCAL_PATH = Path(\"data\")\n",
    "FILE_NAME = \"netflix_reviews.csv\"\n",
    "EXISTING_FILE = \"categorized_reviews.csv\"\n",
    "MODEL_PATH = \"bertopic_netflix_model.pkl\"\n",
    "UNIQUE_COLS = ['review_id', 'datetime']\n",
    "CATEGORY_LOG = \"discovered_categories.txt\"\n",
    "BATCH_SIZE = 20\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")\n",
    "database = os.getenv(\"DB_NAME\")\n",
    "engine = create_engine(f\"mysql+pymysql://{user}:{password}@{host}:{port}/{database}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f611988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORE FUNCTIONS\n",
    "\n",
    "def sync_data():\n",
    "    \"\"\"Fetches the latest dataset from Kaggle via API.\"\"\"\n",
    "    LOCAL_PATH.mkdir(exist_ok=True)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"üì• Downloading latest data from Kaggle...\")\n",
    "    api.dataset_download_files(DATASET, path=LOCAL_PATH, unzip=True, force=True)\n",
    "    \n",
    "    df = pd.read_csv(LOCAL_PATH / FILE_NAME)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7582fc5a-170a-4d2d-b015-d470b13efad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Performs initial schema standardization and data deduplication.\"\"\"\n",
    "    df = df.drop(columns=['reviewCreatedVersion'], errors='ignore')\n",
    "\n",
    "    # 2. Rename Columns\n",
    "    custom_names = {\n",
    "        'reviewId': 'review_id',\n",
    "        'userName': 'user_name',\n",
    "        'content': 'review',\n",
    "        'score': 'rating',\n",
    "        'thumbsUpCount': 'thumbs_up_count',\n",
    "        'at': 'datetime',\n",
    "        'appVersion': 'app_version'\n",
    "    }\n",
    "    # Apply custom names\n",
    "    df = df.rename(columns=custom_names)\n",
    "    \n",
    "    #3  fill content NaNs and \" \" with \"No comment\"\n",
    "    df['review'] = df['review'].fillna(\"No comment\").str.strip()\n",
    "    df.drop_duplicates(subset=UNIQUE_COLS, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e32afb1d-c7d9-4446-bdf8-31424f42c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing():\n",
    "    if os.path.exists(EXISTING_FILE):\n",
    "        df = pd.read_csv(EXISTING_FILE, dtype={'review_id': str})\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        return df\n",
    "    return pd.DataFrame(columns=UNIQUE_COLS + ['review', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ffbdd38-ef13-414e-82c7-11418ef0c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_model():\n",
    "    \"\"\"The NLP Engine: Clusters reviews into topics to be labelled using LLM.\"\"\"\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        return joblib.load(MODEL_PATH)\n",
    "    \n",
    "    print(\"‚ú® Initializing BERTopic Engine...\")\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        min_topic_size=200,\n",
    "        verbose=True\n",
    "    )\n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72812b3a-69cb-4a0c-9dfe-3fc4a6af31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_ollama_for_label(keywords):\n",
    "    \"\"\"Uses Local LLM (Phi-3) to generate human-readable category names.\"\"\"\n",
    "    client = ollama.Client(host='http://127.0.0.1:11434')\n",
    "    prompt = f\"\"\"\n",
    "    Analyze these Netflix review keywords: {\", \".join(keywords)}\n",
    "    \n",
    "    Instruction: Provide a highly specific 2-3 word category name.\n",
    "    STRICT RULE: Do NOT use the words 'General', 'Feedback', 'Netflix', or 'Review'.\n",
    "    Example: Instead of 'General Issues', use 'Streaming Quality' or 'Login Errors'.\n",
    "    \n",
    "    Return ONLY the category name.No full sentences. No explanations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.generate(model='phi3:mini', prompt=prompt, stream=False)\n",
    "        label = response.get('response', '').strip().title()\n",
    "        \n",
    "        return label if label else \"Niche Issue\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Ollama Error: {e}\")\n",
    "        return \"Unclassified Issue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a53e0c-8d0c-4881-965c-cb2247d0462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_sql(df):\n",
    "    def strip_symbols(text):\n",
    "        # Removes leading/trailing symbols but keeps them in the middle\n",
    "        clean_text = re.sub(r'^[\\s\\-\"\\'\\.]+|[\\s\\-\"\\'\\.]+$', '', str(text))\n",
    "        return clean_text.strip().title()\n",
    "\n",
    "    if 'category' in df.columns:\n",
    "        df['category'] = df['category'].apply(strip_symbols)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e87c42b-cd65-4dde-a4b7-036963593bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. Sync and Clean the Raw batch\n",
    "    df_raw = sync_data()\n",
    "    df_raw = clean_data(df_raw)\n",
    "\n",
    "    # 2. Standardize Raw Kaggle Data (YYYY-MM-DD)\n",
    "    df_raw['datetime'] = pd.to_datetime(df_raw['datetime']).dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    # 2. Compare against existing data\n",
    "    df_existing = load_existing()\n",
    "    \n",
    "    if not df_existing.empty:\n",
    "        # Standardize your CSV data (DD/MM/YYYY -> YYYY-MM-DD)\n",
    "        # We use dayfirst=True because your CSV stores it as 16/01/2026\n",
    "        df_existing['datetime'] = pd.to_datetime(\n",
    "            df_existing['datetime'], \n",
    "            dayfirst=True, \n",
    "            errors='coerce'\n",
    "        ).dt.strftime('%Y-%m-%d %H:%M')\n",
    "        \n",
    "        # We perform a 'left merge' on both unique identifiers\n",
    "        # This aligns df_raw against df_existing based on ID and Timestamp\n",
    "        df_merge = pd.merge(\n",
    "            df_raw, \n",
    "            df_existing[UNIQUE_COLS], \n",
    "            on=UNIQUE_COLS, \n",
    "            how='left', \n",
    "            indicator=True\n",
    "        )\n",
    "        \n",
    "        df_to_process = df_merge[df_merge['_merge'] == 'left_only'].drop(columns=['_merge']).copy()\n",
    "    else:\n",
    "        df_to_process = df_raw.copy()\n",
    "        \n",
    "    # ADD THIS LINE FOR TESTING:\n",
    "    #df_to_process = df_to_process.sample(1000)  # Just take 1030 sample rows\n",
    "    #print(f\"Test mode: Processing only {len(df_to_process)} rows.\")\n",
    "\n",
    "    if df_to_process.empty:\n",
    "        print(\"‚úÖ Everything is up to date.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üöÄ Processing {len(df_to_process)} reviews\")\n",
    "    topic_model = get_topic_model()\n",
    "\n",
    "    # Process texts\n",
    "    docs = df_to_process['review'].tolist()\n",
    "\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        # Apply pre-existing categories to new rows\n",
    "        topics, _ = topic_model.transform(docs)\n",
    "    else:\n",
    "        # First-time run: Fit the model to create categories\n",
    "        topics, _ = topic_model.fit_transform(docs)\n",
    "        # Only reduce if -1 exists in topics\n",
    "        if -1 in topics:\n",
    "            print(\"üßπ Reducing outliers to ensure 100% categorization...\")\n",
    "            topics = topic_model.reduce_outliers(docs, topics, strategy=\"embeddings\")\n",
    "        else:\n",
    "            print(\"‚ú® No outliers found in this sample. Skipping reduction.\")\n",
    "        joblib.dump(topic_model, MODEL_PATH)\n",
    "\n",
    "    # label the topics\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    custom_labels = {}\n",
    "\n",
    "    print(\"üè∑Ô∏è Asking Ollama to name the discovered topics...\")\n",
    "    # 4. CONSISTENT LABELING\n",
    "    if not hasattr(topic_model, \"custom_labels_\") or not topic_model.custom_labels_:\n",
    "        topic_model.custom_labels_ = {} \n",
    "        topic_info = topic_model.get_topic_info()\n",
    "\n",
    "        print(\"üè∑Ô∏è Generating INITIAL labels with Ollama...\")\n",
    "        for _, row in tqdm(topic_info.iterrows(), total=len(topic_info), desc=\"Labeling\"):\n",
    "            topic_num = row['Topic']\n",
    "            if topic_num == -1:\n",
    "                topic_model.custom_labels_[topic_num] = \"Outliers/Miscellaneous\"\n",
    "            else:\n",
    "                # Extract keywords for this specific topic\n",
    "                keywords = [word for word, _ in topic_model.get_topic(topic_num)]\n",
    "                # Call Ollama to turn keywords into a readable name\n",
    "                topic_model.custom_labels_[topic_num] = ask_ollama_for_label(keywords)\n",
    "        \n",
    "        # Re-save model with the new labels included\n",
    "        joblib.dump(topic_model, MODEL_PATH)\n",
    "\n",
    "    # 5. Map the topics to the final Category column\n",
    "    df_to_process['category'] = [topic_model.custom_labels_.get(t, \"Miscellaneous\") for t in topics]\n",
    "\n",
    "    #6. rid category colomn of symbols and empty spaces\n",
    "    if not df_to_process.empty:\n",
    "        df_to_process['datetime'] = pd.to_datetime(df_to_process['datetime'], format='mixed', errors='coerce')\n",
    "        df_to_process['unique_ref_id'] = (df_to_process['review_id'].astype(str) + \"_\" + df_to_process['datetime'].dt.strftime('%Y%m%d%H%M'))\n",
    "        df_to_process = df_to_process.drop_duplicates(subset=['unique_ref_id'], keep='first')\n",
    "        \n",
    "        # Apply cleaning\n",
    "        df_to_process = clean_for_sql(df_to_process)\n",
    "\n",
    "    # 7. Save and Append\n",
    "    file_exists = os.path.isfile(EXISTING_FILE)\n",
    "    df_to_process.to_csv(EXISTING_FILE, mode='a', header=not file_exists, index=False)\n",
    "    print(f\"‚úÖ Success! Data saved to {EXISTING_FILE}\")\n",
    "\n",
    "    # 7. Send to SQL\n",
    "    df_to_process.to_sql(\n",
    "    name=\"data\",\n",
    "    con=engine,\n",
    "    if_exists=\"append\",   # or \"replace\"\n",
    "    index=False\n",
    ")\n",
    "    print(f\"Data successfully loaded into database '{database}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6e0c837-638c-4a54-8400-b83c8b75f3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading latest data from Kaggle...\n",
      "Dataset URL: https://www.kaggle.com/datasets/ashishkumarak/netflix-reviews-playstore-daily-updated\n",
      "üöÄ Processing 122 reviews\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb5719cc3bf48a9a0b0e9a2cbba532d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∑Ô∏è Asking Ollama to name the discovered topics...\n",
      "‚úÖ Success! Data saved to categorized_reviews.csv\n",
      "Data successfully loaded into database 'netflix_reviews'\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a94536-85dc-4658-9aec-4c61e0ba820d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
